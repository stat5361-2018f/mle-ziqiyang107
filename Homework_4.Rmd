---
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{bbm}
title: "Statistical Computing Homework 4, Chapter 3"
author: "Ziqi Yang"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    fig_height: 6
    fig_width: 9
    number_sections: yes
    theme: united
    toc: yes
  pdf_document:
    toc: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 3.3.2 Many local maxima
**(a) log-likelihood**
```{r cars}
x <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96,
       2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
log_cos <- function(theta) {
  sum( log(1 - cos(x - theta)) ) - length(x)*log(2*pi)
}
log_cos.plottable <- function(x) {return(sapply(x, log_cos))}
curve(log_cos.plottable, from = -pi, to = pi)
```

**(b) Methods of moment**  
Let
\begin{align*}
\frac{1}{2\pi} \int_{0}^{2\pi} \big[ x - x\cos(x-\theta)\big] dx = \bar{X}
\end{align*}
so evetually, we have $\pi + \sin(\theta)=\bar{X}$, so MOM $\tilde{\theta}_n=\arcsin(\bar{X}-\pi)=$ `r asin(mean(x)-pi)`

**(c) Find MLE using Newton-Raphson**
```{r}
log_cos_D1 <- function(theta) {
  sum( sin(theta-x)/(1-cos(x-theta)) )
}

log_cos_D2 <- function(theta) {
  sum( 1+cos(theta-x)/(1-cos(theta-x))^2 )
}

para <- c(-2.7, 2.7);  temp <- rep(0, length(para))
for (i in 1:length(para)) {
  iter <- 0;  epsilon <- 0.001; 
  temp[i] <- para[i] - 1
  while( (iter <= 1000)&(abs(para[i]-temp[i])/(abs(temp[i])) > epsilon) ) {
    temp[i] <- para[i]
    para[i] <- para[i] - log_cos_D1(para[i])/log_cos_D2(para[i])
    if(abs(para[i]) == Inf) {break}
    iter <- iter + 1
    #print(para[i])
  }
}
print(para)
```

**We can see that if we start from -2.7 and 2.7, the Newton_Raphson algorithm will give us the similar result with the initial values.**  

**(d)**
```{r}
para <- seq(-pi, pi, length.out = 200);  temp <- rep(0, length(para))
for (i in 1:length(para)) {
  iter <- 0;  epsilon <- 0.001; 
  temp[i] <- para[i] - 1
  while( (iter <= 1000)&(abs(para[i]-temp[i])/(abs(temp[i])) > epsilon) ) {
    temp[i] <- para[i]
    para[i] <- para[i] - log_cos_D1(para[i])/log_cos_D2(para[i])
    if(abs(para[i]) == Inf) {break}
    iter <- iter + 1
    #print(para[i])
  }
}
print(para)
#par(pty="s")
curve(log_cos.plottable, from = -pi, to = pi, xlab="theta")
## Allow a second plot on the same graph
par(new=TRUE)

## Plot the second plot and put axis scale on right
plot(seq(-pi, pi, length.out = 200), para, pch=1,  xlab="", ylab="", 
    axes=FALSE, type="b", col="red")

## a little farther out (line=4) to make room for labels
mtext("MLE",side=3,col="red",line=4) 
axis(4, ylim=c(-4,4), col="red",col.axis="red",las=1)


## Add Legend
legend("topleft",legend=c("Log-likelihood","MLE by Newton-Raphson"),
  text.col=c("black","red"),pch=c(16,15),col=c("black","red"))
#par(pty="m")
```

# 3.3.3 Modeling beetle data

```{r}
beet <- data.frame(
    days    = c(0,  8,  28,  41,  63,  69,   97, 117,  135,  154),
    beetles = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024))

growth <- function(x) {
  k <- x[1]; r <- x[2]
  k*beet$beetles[1]/(beet$beetles[1] + (k-beet$beetles[1])*exp(-r*beet$days))
}

growth.contour <- function(k,r) {
  k*beet$beetles[1]/(beet$beetles[1] + (k-beet$beetles[1])*exp(-r*beet$days))
}
growth.error <- function(k, r) {
  sum( (beet$beetles - growth.contour(k, r))^2 )
}
```

**Plot the contour plot to visually check:**  

```{r, echo=FALSE}
k <- seq(0,1500,length.out = 1e4)
r <- seq(0,1,length.out = 100)
z <- outer(k, r, FUN = Vectorize(growth.error))
contour(k, r, z)
```



## Gauss-Newton method
```{r}
library(rootSolve)

#gradient(growth, c(1000, 0.11))

para <- c(500, 0.2);  temp <- c(0, 0); iter <- 0; epsilon <- 1e-5

while( iter <= 10000  &  (sum(para-temp)^2)^0.5 > epsilon ) {
  temp <- para
  grad <- gradient(growth, para)
  para <- para + solve( (t(grad)%*%grad)+diag(0.0001, 2) )%*%t(grad)%*%(beet$beetles-growth(para))
  if(any(para == Inf)) {break}
  iter <- iter + 1
  # print(para)
}

print(para)
#/(sum(temp^2))^0.5 

```

**Above is my own Gauss-Newton method, which use the formula to calculate the optimal parameters, but this depends on the inital values of parameters, so it's not very effect compared with default "nls" function in R**  

```{r}
nls.R <- nls(beetles ~ growth.contour(k,r), data = beet, start = list(k = 100, r = 0.5))
summary(nls.R)
```

**The default method for nlw uses Gauss-Newton, we can see that the result is pretty robust compare with my own Gauss-Newton method**  

**Finally, use self method: Steepest Descent(Gradient Descent)**  

```{r}
growth.error.optim <- function(x) {
  sum( (beet$beetles - growth(x))^2 )
}

iter_num <- 2000
para <- matrix(0, nrow = iter_num, ncol=2, byrow = T)
para[1, ] <- c(100, 0.8); para[2, ] <- para[1,]-0.000001*gradient(growth.error.optim, para[1, ])
iter <- 2
epsilon <- 1e-10

while( iter <= iter_num  &  (sum(para[iter, ]-para[iter-1, ])^2)^0.5 > epsilon ) {
  
  grad.now <- gradient(growth.error.optim, para[iter,])
  grad.old <- gradient(growth.error.optim, para[iter-1,])
  cat("This is Hessian: ", hessian(growth.error.optim, para[iter,]), "\n")
  l <- (para[iter, ]-para[iter-1,]) %*% t(grad.now-grad.old)/sum((grad.now-grad.old)^2)
  cat("This is l: ", l, "\n")
  para[iter+1, ] <- para[iter, ] - l %*% grad.now

  if(any(para[iter, ] == Inf)) {break}
  iter <- iter + 1
  cat("This current parameter: ", para[iter, ], "\n")
}
print(para[iter, ])
```

**The final gradient descent estimate for $K,r$ are `r print(para[iter, ])`. My own Gradient Descent worked not very well, due to nearly singularity of Hessian matrix.**  

## Multivariate method based on log-normal model MLE: use BFGS
```{r}
m_log.mle.function <- function(x) {
  -sum( dnorm(log(beet$beetles), mean = log(growth(x[1:2])), sd = x[3], log = TRUE) )
}
optim(c(1000, 0.1, 1), m_log.mle.function, lower=c(0,0,0), upper=c(Inf, 1, Inf), method = "L-BFGS-B")$par

```

**Above result is MLE for $(K, r, \sigma^2)$. But actually this MLE method depends heavily on the initial value, I tried several, but could not get to the close result with Gauss-Newton one. **  









